# -*- coding: utf-8 -*-
"""trying

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wAaqIUOl48uOyr1hRx9knjZuAekkWi39
"""

import torch
import pandas as pd
from sklearn.model_selection import train_test_split
!pip install transformers

from transformers import AutoTokenizer, AutoModel
from torch.utils.data import TensorDataset, DataLoader

!pip install datasets

from datasets import Dataset
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments
import numpy as np

!pip install evaluate

import evaluate
from transformers import TrainingArguments, Trainer
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import seaborn as sns

plt.style.use('ggplot')

from scipy.stats import norm
from sklearn.preprocessing import StandardScaler
from scipy import stats

df = pd.read_csv('/content/dataset.csv').drop(['Unnamed: 0'], axis=1)

df = df.rename(columns={'Tweets': 'text', 'Class': 'label'})

train, validation = train_test_split(df, test_size=0.2, random_state=42)

df.head()

df.shape

df["label"].unique()

class_df = df.groupby('label').count()['text'].reset_index().sort_values(by='text',ascending=False)
class_df.style.background_gradient(cmap='winter')

percent_class=class_df.text
labels= class_df.label

colors = ['#17C37B','#F92969','#FACA0C']

my_pie,_,_ = plt.pie(percent_class,radius = 1.2,labels=labels,colors=colors,autopct="%.1f%%")

plt.setp(my_pie, width=0.6, edgecolor='white')
plt.savefig('Label Distribution.pdf', format='pdf', bbox_inches='tight')
plt.show()

train_dataset = Dataset.from_pandas(train)
test_dataset = Dataset.from_pandas(validation)

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns(["text","__index_level_0__"]).rename_column("label", "labels")
test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns(["text","__index_level_0__"]).rename_column("label", "labels")
train_dataset.set_format("torch")
test_dataset.set_format("torch")

train_dataset

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(test_dataset, batch_size=8)

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased",num_labels=2)

from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5)

from transformers import get_scheduler
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

import torch
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
print("")

model.config.problem_type = "multi_label_classification"

from tqdm.auto import tqdm
import torch.nn.functional as F
import evaluate
from sklearn.metrics import classification_report
progress_bar = tqdm(range(num_training_steps))


for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        batch["labels"] = torch.nn.functional.one_hot(batch["labels"].to(torch.int64), 2).to(torch.float)

        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()


        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    model.eval()
    true_labels = []
    predicted_labels = []
    metric = evaluate.load("accuracy")
    for batch in eval_dataloader:
        batch["labels"] = torch.nn.functional.one_hot(batch["labels"].to(torch.int64), 2).to(torch.float)

        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        batch["labels"] = torch.argmax(batch["labels"],dim=-1).to(torch.int32)
        true_labels.extend(batch["labels"].tolist())
        predicted_labels.extend(predictions.tolist())

        metric.add_batch(predictions=predictions, references=batch["labels"])
    print(metric.compute())

from sklearn.metrics import classification_report

report = classification_report(true_labels, predicted_labels)
print('\tClassification Report for BERT:\n\n',report)

train_dataset = Dataset.from_pandas(train)
test_dataset = Dataset.from_pandas(validation)

from transformers import RobertaTokenizer
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns(["text","__index_level_0__"]).rename_column("label", "labels")
test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns(["text","__index_level_0__"]).rename_column("label", "labels")
train_dataset.set_format("torch")
test_dataset.set_format("torch")

train_dataset

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(test_dataset, batch_size=8)

model = AutoModelForSequenceClassification.from_pretrained("roberta-base",num_labels=2)

from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5)

from transformers import get_scheduler
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

import torch
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
print("")

model.config.problem_type = "multi_label_classification"

from tqdm.auto import tqdm
import torch.nn.functional as F
import evaluate
from sklearn.metrics import classification_report
progress_bar = tqdm(range(num_training_steps))


for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        batch["labels"] = torch.nn.functional.one_hot(batch["labels"].to(torch.int64), 2).to(torch.float)

        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()


        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    model.eval()
    true_labels = []
    predicted_labels = []
    metric = evaluate.load("accuracy")
    for batch in eval_dataloader:
        batch["labels"] = torch.nn.functional.one_hot(batch["labels"].to(torch.int64), 2).to(torch.float)

        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        batch["labels"] = torch.argmax(batch["labels"],dim=-1).to(torch.int32)
        true_labels.extend(batch["labels"].tolist())
        predicted_labels.extend(predictions.tolist())

        metric.add_batch(predictions=predictions, references=batch["labels"])
    print(metric.compute())

from sklearn.metrics import classification_report

report = classification_report(true_labels, predicted_labels)
print('\tClassification Report for Roberta:\n\n',report)

train_dataset = Dataset.from_pandas(train)
test_dataset = Dataset.from_pandas(validation)

from transformers import AutoTokenizer, DistilBertForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True).remove_columns(["text","__index_level_0__"]).rename_column("label", "labels")
test_dataset = test_dataset.map(tokenize_function, batched=True).remove_columns(["text","__index_level_0__"]).rename_column("label", "labels")
train_dataset.set_format("torch")
test_dataset.set_format("torch")

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(test_dataset, batch_size=8)

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5)

from transformers import get_scheduler
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

import torch
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
print("")

model.config.problem_type = "multi_label_classification"

from tqdm.auto import tqdm
import torch.nn.functional as F
import evaluate
from sklearn.metrics import classification_report
progress_bar = tqdm(range(num_training_steps))


for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        batch["labels"] = torch.nn.functional.one_hot(batch["labels"].to(torch.int64), 2).to(torch.float)

        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()


        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    model.eval()
    true_labels = []
    predicted_labels = []
    metric = evaluate.load("accuracy")
    for batch in eval_dataloader:
        batch["labels"] = torch.nn.functional.one_hot(batch["labels"].to(torch.int64), 2).to(torch.float)

        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        batch["labels"] = torch.argmax(batch["labels"],dim=-1).to(torch.int32)
        true_labels.extend(batch["labels"].tolist())
        predicted_labels.extend(predictions.tolist())

        metric.add_batch(predictions=predictions, references=batch["labels"])
    print(metric.compute())

from sklearn.metrics import classification_report

report = classification_report(true_labels, predicted_labels)
print('\tClassification Report for Distilbert:\n\n',report)

